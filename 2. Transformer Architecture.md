# Transformer Architecture – Simple Explanation

---

## Overview

The **Transformer architecture** was originally introduced for **sequence-to-sequence tasks** such as **machine translation**, where one sequence (input sentence) is transformed into another sequence (output sentence).

Unlike older architectures like **Recurrent Neural Networks (RNNs)** or **Convolutional Neural Networks (CNNs)**, Transformers **do not process data sequentially**. Instead, they rely entirely on **attention mechanisms**, especially **self-attention**, to understand relationships between words in a sentence.

At a very high level, you can think of a Transformer as a **black box**:

* You provide a sentence in one language (e.g., English)
* The Transformer produces the translated sentence in another language (e.g., Spanish)

---

## High-Level Transformer Workflow

```text
Input Sentence (English)
        ↓
      Encoder
        ↓
  Contextual Representation
        ↓
      Decoder
        ↓
Output Sentence (Spanish)
```

The Transformer is composed of **two major components**:

* **Encoder** – understands the input sentence
* **Decoder** – generates the output sentence

---

## Encoder–Decoder Architecture

The Transformer follows a **classic encoder–decoder architecture**, meaning the model is divided into two distinct but connected parts.

---

## 1. Encoder

### What Does the Encoder Do?

The **encoder** reads the input sentence and converts it into a **numerical representation** that captures the meaning and context of each word.

Example:

```text
Input Sentence: "How are you?"
```

The encoder transforms this sentence into a **matrix of vectors**, where:

* Each word is represented as a vector
* Each vector contains contextual information about the word

This means the word **"are"** knows that it is related to **"How"** and **"you"**, not just standing alone.

### Key Purpose of the Encoder

* Understand the **semantic meaning** of the sentence
* Capture **relationships between all words** using self-attention
* Produce a **context-rich representation** for the decoder

---

## 2. Decoder

### What Does the Decoder Do?

The **decoder** generates the output sentence **one word at a time**, using:

* The encoder’s output (understanding of the input)
* The words it has already generated

Example:

```text
Encoder Output → "How are you?"
Decoder Output → "¿Cómo estás?"
```

The decoder predicts:

1. First word
2. Second word
3. Third word

Each prediction depends on:

* The encoder’s representation
* Previously generated words

---

## Stacked Layers: Why Multiple Encoders and Decoders?

The encoder and decoder are **not single blocks**. They are **stacks of identical layers**.

---

## Encoder Stack

* The encoder consists of **multiple encoder layers** stacked on top of each other
* Each layer refines the understanding of the sentence
* Output of one encoder layer is passed as input to the next

```text
Input Embeddings
        ↓
Encoder Layer 1
        ↓
Encoder Layer 2
        ↓
Encoder Layer 3
        ↓
     ...
        ↓
Final Encoder Output
```

---

## Decoder Stack

* The decoder also consists of **multiple decoder layers**
* Each decoder layer:

  * Uses the **encoder’s final output**
  * Uses **previous decoder outputs**

```text
Previous Output Tokens
        ↓
Decoder Layer 1
        ↓
Decoder Layer 2
        ↓
Decoder Layer 3
        ↓
     ...
        ↓
Final Output Tokens
```

---

## Original Transformer Configuration

In the original Transformer paper:

| Component | Number of Layers |
| --------- | ---------------- |
| Encoder   | 6 layers         |
| Decoder   | 6 layers         |

However, this is **not fixed**.

---

## Flexible Depth (N Layers)

The Transformer can be configured with:

* **N encoder layers**
* **N decoder layers**

Depending on:

* Task complexity
* Dataset size
* Computational resources

```text
More layers → Deeper understanding (usually)
Fewer layers → Faster but less expressive model
```

---

## Key Ideas to Remember

* **Encoder** → Understands the input sentence
* **Decoder** → Generates the output sentence
* Both are **stacks of layers**, not single blocks
* Attention replaces recurrence and convolution
* Deeper stacks usually lead to better performance

---

## Summary

* Transformers process sequences using **attention mechanisms**
* They follow an **encoder–decoder structure**
* Encoders create contextual representations
* Decoders generate output step by step
* Multiple layers allow **progressive refinement** of meaning

This forms the **core foundation** of the Transformer architecture.
