# Encoder–Decoder Multi-Head Attention (Cross Attention)

A **clear, beginner-friendly, and confusion-free explanation** of **Encoder–Decoder (Cross) Attention** in Transformers.
This README is designed to be **GitHub-ready**, cleanly structured, and easy to revise or download.

---

## Table of Contents

1. What Problem Cross Attention Solves
2. Where Cross Attention Lives in the Transformer
3. The One Golden Rule (Very Important)
4. Encoder vs Decoder — Roles Explained
5. Queries, Keys, and Values (No Confusion)
6. Step-by-Step Cross Attention Flow
7. Why It Is Called *Cross* Attention
8. Multi-Head Cross Attention
9. Decoder Layer — Full Flow
10. Feed-Forward Neural Network (Decoder)
11. One-Line Mental Model
12. Common Mistakes to Avoid

---

## 1. What Problem Cross Attention Solves

Transformers are often used for **sequence-to-sequence tasks**, such as:

* Machine Translation
* Text Summarization
* Question Answering

In these tasks:

* The **encoder** reads the *input sequence*
* The **decoder** generates the *output sequence* step by step

❓ **Problem:**
How does the decoder know *which parts of the input sentence are relevant* while generating each output word?

✅ **Answer:** Encoder–Decoder Attention (Cross Attention)

---

## 2. Where Cross Attention Lives in the Transformer

Each **decoder layer** contains **three sub-layers**:

1. Masked Self-Attention
2. **Encoder–Decoder Attention (Cross Attention)**
3. Feed-Forward Neural Network

Cross attention is the **bridge** between the encoder and the decoder.

---

## 3. The One Golden Rule (Very Important)

> **Decoder asks questions. Encoder provides answers.**

If you remember only one thing, remember this.

---

## 4. Encoder vs Decoder — Roles Explained

| Component | Responsibility                                    |
| --------- | ------------------------------------------------- |
| Encoder   | Reads and understands the entire input sequence   |
| Decoder   | Generates the output sequence one token at a time |

* The encoder finishes its job **first**
* The decoder uses encoder output **while generating each word**

---

## 5. Queries, Keys, and Values (No Confusion)

This is the most common source of confusion. Let’s fix it permanently.

### Correct Source of Q, K, V in Cross Attention

| Element       | Comes From | Meaning                                  |
| ------------- | ---------- | ---------------------------------------- |
| **Query (Q)** | Decoder    | What am I trying to generate right now?  |
| **Key (K)**   | Encoder    | What input information exists?           |
| **Value (V)** | Encoder    | What is the actual content of the input? |

### Key Takeaway

* ❌ Encoder **never** provides queries
* ✅ Decoder **always** provides queries

---

## 6. Step-by-Step Cross Attention Flow

### Example

**Input (Encoder):**

```
I love machine learning
```

**Output (Decoder, being generated):**

```
J’aime l’apprentissage automatique
```

### Step-by-Step

1. Encoder converts every input word into contextual vectors
2. Decoder generates output **one word at a time**
3. For the current output word:

   * Decoder creates a **Query**
   * Encoder provides **Keys and Values**
4. Decoder compares its query with encoder keys
5. Decoder pulls relevant information from encoder values

This lookup process is **cross attention**.

---

## 7. Why It Is Called *Cross* Attention

| Attention Type  | What It Attends To                      |
| --------------- | --------------------------------------- |
| Self-Attention  | Same sequence                           |
| Cross Attention | Different sequences (decoder → encoder) |

Cross attention connects:

* Output sequence → Input sequence

---

## 8. Multi-Head Cross Attention

Instead of a single attention operation:

* Cross attention is split into **multiple heads**
* Each head learns a different alignment

### Why Multiple Heads?

* One head focuses on word-level alignment
* Another focuses on phrase-level meaning
* Another captures long-range dependencies

All heads are:

1. Computed in parallel
2. Concatenated
3. Linearly projected

---

## 9. Decoder Layer — Full Flow

Each decoder layer works in this order:

1. **Masked Self-Attention**
   Looks only at previously generated tokens

2. **Encoder–Decoder (Cross) Attention**
   Looks at encoder output to find relevant input information

3. **Feed-Forward Neural Network**
   Processes the information independently at each position

---

## 10. Feed-Forward Neural Network (Decoder)

The feed-forward network:

* Is identical to the encoder FFN
* Is applied **independently to each token**
* Does **not** mix information between tokens

### Structure

```
FFN(x) = max(0, xW1 + b1)W2 + b2
```

### Purpose

* Attention gathers information
* FFN **processes and refines** that information

---

## 11. One-Line Mental Model

> **Masked Self-Attention:** What have I written so far?
> **Cross Attention:** What should I look at in the input?
> **Feed-Forward:** How do I transform this information?

---

## 12. Common Mistakes to Avoid

❌ Thinking encoder provides queries
❌ Mixing self-attention with cross attention
❌ Assuming FFN performs attention

✅ Decoder asks, encoder answers
✅ Cross attention connects input and output
✅ FFN is pure transformation

---

## Final Summary

* Cross attention allows the decoder to **focus on relevant input tokens**
* Queries come from the decoder
* Keys and values come from the encoder
* This mechanism enables accurate sequence-to-sequence generation

---

**You now have a complete, correct, and confusion-free understanding of Encoder–Decoder Cross Attention.**
