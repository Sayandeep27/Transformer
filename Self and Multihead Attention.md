# Attention, Self-Attention & Multi-Head Attention ‚Äî Complete Beginner-to-Advanced Guide

---

## üìå Overview

This README provides a **clear, structured, and in-depth explanation** of:

* **Attention**
* **Self-Attention**
* **Multi-Head Attention**

These concepts form the **core of Transformer architectures** such as BERT, GPT, and T5. The explanation progresses **step-by-step**, using **intuition, examples, equations, and code**, ensuring nothing is skipped.

---

## üö© Why Attention Was Introduced

### The Problem with Earlier Models (RNNs / LSTMs)

* Process tokens **sequentially**
* Hard to capture **long-range dependencies**
* Information fades over long sequences
* Not easily parallelizable

Example:

> "The animal didn‚Äôt cross the street because it was tired"

What does **"it"** refer to?

Traditional models struggle to connect distant words.

---

## üéØ What Is Attention?

### Definition

**Attention is a mechanism that allows a model to focus on the most relevant parts of an input sequence when processing each element.**

### Core Idea

For each word:

> ‚ÄúWhich other words should I pay attention to, and how much?‚Äù

---

## üß† Intuition Behind Attention

Attention works like an **information retrieval system**:

| Component | Meaning                 |
| --------- | ----------------------- |
| Query (Q) | What am I looking for?  |
| Key (K)   | What do I contain?      |
| Value (V) | The information I carry |

---

## üîÅ What Is Self-Attention?

### Why "Self"?

* The sequence attends **to itself**
* Each token looks at **every other token** in the same sequence

---

## üß© Step-by-Step Self-Attention (With Example)

### Example Sentence

```
I love deep learning
```

Tokens:

```
[I] [love] [deep] [learning]
```

---

### Step 1: Embeddings

Each token is converted into a vector embedding.

---

### Step 2: Create Query, Key, Value

Each embedding is projected into **three vectors**:

```
Q = X ¬∑ W_Q
K = X ¬∑ W_K
V = X ¬∑ W_V
```

---

### Step 3: Compute Attention Scores

For token *i* attending to token *j*:

```
score(i, j) = Q_i ¬∑ K_j
```

Scaled version:

```
scaled_score = (Q ¬∑ K·µÄ) / ‚àöd_k
```

**Why scaling?**

* Prevents large dot-product values
* Stabilizes gradients during training

---

### Step 4: Apply Softmax

Softmax converts scores into probabilities:

| Token    | Weight |
| -------- | ------ |
| I        | 0.20   |
| love     | 0.45   |
| deep     | 0.15   |
| learning | 0.20   |

All weights sum to **1**.

---

### Step 5: Weighted Sum of Values

```
Output = Œ£ (attention_weight √ó V)
```

This produces a **context-aware representation** for each word.

---

## üìê Self-Attention Formula

```
Attention(Q, K, V) = softmax(QK·µÄ / ‚àöd_k) V
```

---

## üöÄ Why Self-Attention Is Powerful

* Captures **long-range dependencies**
* Fully **parallelizable**
* No sequential bottleneck
* Each word understands global context

---

## üîÄ What Is Multi-Head Attention?

### Motivation

A single attention mechanism can only capture **one type of relationship**.

Language contains multiple relationships:

* Syntax
* Semantics
* Positional relevance
* Subject-object links

---

## üß† Multi-Head Attention Explained

Instead of one attention:

‚û°Ô∏è Use **multiple attention heads in parallel**

Each head:

* Learns different relationships
* Has its own Q, K, V projections

---

### Multi-Head Attention Workflow

1. Input embeddings
2. Split into **h heads**
3. Each head performs self-attention
4. Outputs are concatenated
5. Final linear projection

---

## üìê Mathematical Representation

### For One Head

```
head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
```

### Multi-Head Attention

```
MultiHead(Q, K, V) = Concat(head‚ÇÅ, head‚ÇÇ, ..., head_h) W_O
```

---

## üé≠ Intuition: Human Analogy

Imagine a group discussion:

* One person listens for **grammar**
* One for **meaning**
* One for **context**
* One for **long-distance references**

All perspectives are combined ‚Üí **final understanding**

---

## üß™ PyTorch Implementation (Minimal)

```python
import torch
import torch.nn.functional as F

def scaled_dot_product_attention(Q, K, V):
    d_k = Q.size(-1)
    scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_k ** 0.5)
    weights = F.softmax(scores, dim=-1)
    return torch.matmul(weights, V)
```

---

## üß† Key Takeaways

| Concept              | One-Line Meaning              |
| -------------------- | ----------------------------- |
| Attention            | Focus on relevant tokens      |
| Self-Attention       | Tokens attend to each other   |
| Multi-Head Attention | Multiple perspectives at once |

---

## üìå Final Mental Model

> **Attention** ‚Üí What matters?

> **Self-Attention** ‚Üí Words understanding words

> **Multi-Head Attention** ‚Üí Understanding from many viewpoints

---

## üì• Usage

This README is **GitHub-ready** and suitable for:

* ML / DL study notes
* Interview preparation
* Teaching materials
* Transformer implementation references

You can **copy, edit, or download** directly from the canvas.

---

## ‚≠ê If You Want Next

* Encoder vs Decoder attention
* Masked self-attention (GPT)
* BERT vs GPT attention flow
* Full Transformer from scratch

Just continue the conversation.
