# Transformer Decoder Layer – End-to-End Explanation

A **professional, GitHub-ready README** explaining the **Transformer Decoder layer** step by step, from inputs to final token prediction. This document is written to remove confusion and build *true intuition*, not just formulas.

---

## Table of Contents

1. Introduction
2. What the Decoder Does (Big Picture)
3. Decoder vs Encoder (Quick Comparison)
4. Inputs to the Decoder
5. High-Level Decoder Layer Architecture
6. Step 1: Masked Multi-Head Self-Attention
7. Step 2: Encoder–Decoder (Cross) Attention
8. Step 3: Feed-Forward Neural Network (FFN)
9. Residual Connections and Layer Normalization
10. Stacking Multiple Decoder Layers
11. Final Linear + Softmax Output Layer
12. Training vs Inference in the Decoder
13. End-to-End Decoder Flow Summary
14. Key Intuitions to Remember

---

## 1. Introduction

The **Transformer Decoder** is responsible for **generating output tokens one by one**, using:

* Previously generated tokens (autoregressive behavior)
* The encoder’s understanding of the input sequence

The decoder is the core reason Transformers can perform tasks such as:

* Machine Translation
* Text Generation
* Summarization
* Chat-based LLM responses

This README explains the **entire decoder layer in detail**, without skipping *any* conceptual or mathematical step.

---

## 2. What the Decoder Does (Big Picture)

At a high level, the decoder answers this question:

> “Given what I have already generated and what the input sentence means, what should the next word be?”

The decoder:

1. Reads its **own past outputs**
2. Looks at the **encoder output**
3. Predicts the **next token**

This happens **token by token**, not all at once.

---

## 3. Decoder vs Encoder (Quick Comparison)

| Feature                         | Encoder          | Decoder           |
| ------------------------------- | ---------------- | ----------------- |
| Processes full sequence at once | Yes              | No                |
| Uses masking                    | No               | Yes (causal mask) |
| Sees future tokens              | Yes              | No                |
| Uses cross-attention            | No               | Yes               |
| Purpose                         | Understand input | Generate output   |

---

## 4. Inputs to the Decoder

The decoder receives **two inputs**:

### 4.1 Shifted Target Tokens

If the target sentence is:

```
I love NLP
```

The decoder input during training becomes:

```
<START> I love
```

This shifting ensures the decoder **never sees the token it is supposed to predict**.

---

### 4.2 Encoder Output

The encoder processes the source sentence and produces contextual embeddings:

```
E = [e₁, e₂, e₃, ..., eₙ]
```

These embeddings:

* Contain full input context
* Remain **fixed** throughout all decoder layers

---

## 5. High-Level Decoder Layer Architecture

Each **decoder layer** consists of **three sub-layers**:

1. Masked Multi-Head Self-Attention
2. Encoder–Decoder (Cross) Attention
3. Feed-Forward Neural Network (FFN)

Each sub-layer is wrapped with:

* Residual connection
* Layer normalization

---

## 6. Step 1: Masked Multi-Head Self-Attention

### 6.1 Why Masking Is Necessary

The decoder **must not look at future tokens**.

Example:

```
I love ___
```

While predicting the blank, the decoder must only use:

```
I love
```

If future tokens were visible, the model would cheat.

---

### 6.2 Self-Attention Computation

Given decoder embeddings `D`:

```
Q = D · WQ
K = D · WK
V = D · WV
```

Attention scores:

```
Scores = QKᵀ / √dₖ
```

---

### 6.3 Causal (Look-Ahead) Mask

The mask blocks future positions:

```
[ 0   -∞   -∞ ]
[ 0    0   -∞ ]
[ 0    0    0 ]
```

After softmax:

* Future tokens get probability **0**

---

### 6.4 Multi-Head Attention

Multiple attention heads allow the decoder to learn:

* Grammar relationships
* Long-range dependencies
* Semantic emphasis

Each head works independently, then outputs are concatenated.

---

### 6.5 Output of Step 1

The decoder now understands:

* Relationships between **previously generated tokens**
* But **has not used input sentence information yet**

---

## 7. Step 2: Encoder–Decoder (Cross) Attention

### 7.1 Purpose of Cross-Attention

Cross-attention answers:

> “Which parts of the input sentence are relevant right now?”

---

### 7.2 Query, Key, Value Sources

| Component | Comes From                 |
| --------- | -------------------------- |
| Query (Q) | Decoder output from Step 1 |
| Key (K)   | Encoder output             |
| Value (V) | Encoder output             |

---

### 7.3 Attention Formula

```
Attention(Q, K, V) = softmax(QKᵀ / √dₖ) · V
```

This aligns **output tokens** with **input tokens**.

---

### 7.4 Intuition Example

While translating:

```
I love NLP → J’aime le NLP
```

When generating `aime`, the decoder focuses on:

```
love
```

Cross-attention creates this alignment.

---

### 7.5 Output of Step 2

Each decoder token now contains:

* Context from previous outputs
* Relevant information from the input sentence

---

## 8. Step 3: Feed-Forward Neural Network (FFN)

### 8.1 FFN Structure

Applied independently to each token:

```
FFN(x) = max(0, xW₁ + b₁)W₂ + b₂
```

Typical dimensions:

* `d_model → d_ff → d_model`
* `d_ff` is much larger

---

### 8.2 Why FFN Is Needed

| Component | Role                                   |
| --------- | -------------------------------------- |
| Attention | Mixes information across tokens        |
| FFN       | Adds depth and non-linearity per token |

FFN answers:

> “How should I transform this information?”

---

## 9. Residual Connections and Layer Normalization

Each sub-layer follows this pattern:

```
Output = LayerNorm(Input + SubLayer(Input))
```

Benefits:

* Stable gradients
* Faster convergence
* Preserves original signal

---

## 10. Stacking Multiple Decoder Layers

Decoder layers are stacked (N times):

| Model                | Decoder Layers |
| -------------------- | -------------- |
| Original Transformer | 6              |
| GPT-2                | 12             |
| GPT-3                | 96             |

Each layer:

* Refines alignment
* Improves abstraction

---

## 11. Final Linear + Softmax Output Layer

After the last decoder layer:

### 11.1 Linear Projection

```
logits = X · W_vocab
```

---

### 11.2 Softmax

```
P(token) = softmax(logits)
```

The next token is selected via:

* Greedy decoding
* Beam search
* Sampling

---

## 12. Training vs Inference in the Decoder

| Aspect      | Training              | Inference                   |
| ----------- | --------------------- | --------------------------- |
| Input       | Shifted target tokens | Previously generated tokens |
| Parallelism | Yes                   | No                          |
| Masking     | Required              | Required                    |

---

## 13. End-to-End Decoder Flow Summary

```
Shifted target tokens
 → Masked Self-Attention
 → Cross-Attention
 → Feed-Forward Network
 → Repeat N times
 → Linear + Softmax
 → Next token
```

---

## 14. Key Intuitions to Remember

* Masked Self-Attention → *What have I already said?*
* Cross-Attention → *Which input words matter now?*
* FFN → *How should I transform this meaning?*

---

## Final Note

If you fully understand this decoder flow, you understand the **core mechanism behind GPT, ChatGPT, and modern LLMs**.

This README is **download-ready**, **GitHub-ready**, and suitable for **revision, teaching, or interviews**.
